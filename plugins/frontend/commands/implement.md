---
description: Full-cycle feature implementation with multi-agent orchestration and quality gates
allowed-tools: Task, AskUserQuestion, Bash, Read, TodoWrite, Glob, Grep
---

## Mission

Orchestrate a complete feature implementation workflow using specialized agents with built-in quality gates and feedback loops. This command manages the entire lifecycle from architecture planning through implementation, code review, testing, user approval, and project cleanup.

## CRITICAL: Orchestrator Constraints

**You are an ORCHESTRATOR, not an IMPLEMENTER.**

**‚úÖ You MUST:**
- Use Task tool to delegate ALL implementation work to agents
- Use Bash to run git commands (status, diff, log)
- Use Read/Glob/Grep to understand context
- Use TodoWrite to track workflow progress
- Use AskUserQuestion for user approval gates
- Coordinate agent workflows and feedback loops

**‚ùå You MUST NOT:**
- Write or edit ANY code files directly (no Write, no Edit tools)
- Implement features yourself
- Fix bugs yourself
- Create new files yourself
- Modify existing code yourself
- "Quickly fix" small issues - always delegate to developer

**Delegation Rules:**
- ALL code changes ‚Üí developer agent
- ALL planning ‚Üí architect agent
- ALL design reviews (UI fidelity) ‚Üí designer agent
- ALL UI implementation/fixes ‚Üí ui-developer agent
- ALL code reviews ‚Üí reviewer + codex-reviewer agents
- ALL testing ‚Üí test-architect agent
- ALL cleanup ‚Üí cleaner agent
- OPTIONAL Codex UI expert review ‚Üí ui-developer-codex agent

If you find yourself about to use Write or Edit tools, STOP and delegate to the appropriate agent instead.

## Feature Request

$ARGUMENTS

## Multi-Agent Orchestration Workflow

### PRELIMINARY: Check for Code Analysis Tools (Recommended)

**Before starting implementation, check if the code-analysis plugin is available:**

Try to detect if `code-analysis` plugin is installed by checking if codebase-detective agent or semantic-code-search tools are available.

**If code-analysis plugin is NOT available:**

Inform the user with this message:

```
üí° Recommendation: Install Code Analysis Plugin

For best results investigating existing code patterns, components, and architecture,
we recommend installing the code-analysis plugin.

Benefits:
- üîç Semantic code search (find components by functionality, not just name)
- üïµÔ∏è Codebase detective agent (understand existing patterns)
- üìä 40% faster codebase investigation
- üéØ Better understanding of where to integrate new features

Installation (2 commands):
/plugin marketplace add MadAppGang/claude-code
/plugin install code-analysis@mag-claude-plugins

Repository: https://github.com/MadAppGang/claude-code

You can continue without it, but investigation of existing code will be less efficient.
```

**If code-analysis plugin IS available:**

Great! You can use the codebase-detective agent and semantic-code-search skill during
architecture planning to investigate existing patterns and find the best integration points.

**Then proceed with the implementation workflow regardless of plugin availability.**

---

### STEP 0: Initialize Global Workflow Todo List (MANDATORY FIRST STEP)

**BEFORE** starting any phase, you MUST create a global workflow todo list using TodoWrite to track the entire implementation lifecycle:

```
TodoWrite with the following items:
- content: "PHASE 1: Launch architect for architecture planning"
  status: "in_progress"
  activeForm: "PHASE 1: Launching architect for architecture planning"
- content: "PHASE 1: User approval gate - wait for plan approval"
  status: "pending"
  activeForm: "PHASE 1: Waiting for user approval of architecture plan"
- content: "PHASE 2: Launch developer for implementation"
  status: "pending"
  activeForm: "PHASE 2: Launching developer for implementation"
- content: "PHASE 2: Get manual testing instructions from implementation agent"
  status: "pending"
  activeForm: "PHASE 2: Getting manual testing instructions from implementation agent"
- content: "PHASE 2.5: Detect Figma design links in feature request and plan"
  status: "pending"
  activeForm: "PHASE 2.5: Detecting Figma design links"
- content: "PHASE 2.5: Run design fidelity validation for UI components (if Figma links found)"
  status: "pending"
  activeForm: "PHASE 2.5: Running design fidelity validation"
- content: "PHASE 2.5: Quality gate - ensure UI matches design specifications"
  status: "pending"
  activeForm: "PHASE 2.5: Ensuring UI matches design specifications"
- content: "PHASE 2.5: User manual validation of UI components (conditional - if enabled)"
  status: "pending"
  activeForm: "PHASE 2.5: User validation of UI components"
- content: "PHASE 3: Launch ALL THREE reviewers in parallel (code + codex + UI testing)"
  status: "pending"
  activeForm: "PHASE 3: Launching all three reviewers in parallel"
- content: "PHASE 3: Analyze triple review results and determine if fixes needed"
  status: "pending"
  activeForm: "PHASE 3: Analyzing triple review results"
- content: "PHASE 3: Quality gate - ensure all three reviewers approved"
  status: "pending"
  activeForm: "PHASE 3: Ensuring all three reviewers approved"
- content: "PHASE 4: Launch test-architect for test implementation"
  status: "pending"
  activeForm: "PHASE 4: Launching test-architect for test implementation"
- content: "PHASE 4: Quality gate - ensure all tests pass"
  status: "pending"
  activeForm: "PHASE 4: Ensuring all tests pass"
- content: "PHASE 5: User approval gate - present implementation for final review"
  status: "pending"
  activeForm: "PHASE 5: Presenting implementation for user final review"
- content: "PHASE 5: Launch cleaner to clean up temporary artifacts"
  status: "pending"
  activeForm: "PHASE 5: Launching cleaner to clean up temporary artifacts"
- content: "PHASE 6: Generate comprehensive final summary"
  status: "pending"
  activeForm: "PHASE 6: Generating comprehensive final summary"
- content: "PHASE 6: Present summary and complete user handoff"
  status: "pending"
  activeForm: "PHASE 6: Presenting summary and completing user handoff"
```

**Update this global todo list** as you progress through each phase:
- Mark items as "completed" immediately after finishing each step
- Mark the next item as "in_progress" before starting it
- Add additional items for feedback loops (e.g., "PHASE 3 - Iteration 2: Re-run reviewers after fixes")
- Track the number of review cycles and test cycles by adding iteration tasks

**IMPORTANT**: This global todo list provides high-level workflow tracking. Each agent will also maintain its own internal todo list for detailed task tracking.

### PHASE 1: Architecture Planning (architect)

1. **Launch Planning Agent**:
   - **Update TodoWrite**: Ensure "PHASE 1: Launch architect" is marked as in_progress
   - Use Task tool with `subagent_type: frontend:architect`
   - Provide the feature request: $ARGUMENTS
   - Agent will perform gap analysis and ask clarifying questions
   - Agent will create comprehensive plan in AI-DOCS/
   - **Update TodoWrite**: Mark "PHASE 1: Launch architect" as completed

2. **User Approval Gate**:
   - **Update TodoWrite**: Mark "PHASE 1: User approval gate" as in_progress
   - Present the plan to the user clearly
   - Use AskUserQuestion to ask: "Are you satisfied with this architecture plan?"
   - Options: "Yes, proceed to implementation" / "No, I have feedback"

3. **Feedback Loop**:
   - IF user not satisfied:
     * Collect specific feedback
     * **Update TodoWrite**: Add "PHASE 1 - Iteration X: Re-run planner with feedback" task
     * Re-run architect with feedback
     * Repeat approval gate
   - IF user satisfied:
     * **Update TodoWrite**: Mark "PHASE 1: User approval gate" as completed
     * Proceed to Phase 2
   - **DO NOT proceed without user approval**

### PHASE 2: Implementation (developer)

1. **Launch Implementation Agent**:
   - **Update TodoWrite**: Mark "PHASE 2: Launch developer" as in_progress
   - Use Task tool with `subagent_type: frontend:developer`
   - Provide:
     * Path to approved plan documentation in AI-DOCS/
     * Clear instruction to follow the plan step-by-step
     * Guidance to write proper documentation
     * Instruction to ask for advice if obstacles are encountered

2. **Implementation Monitoring**:
   - Agent implements features following the plan
   - Agent should document decisions and patterns used
   - If agent encounters blocking issues, it should report them and request guidance
   - **Update TodoWrite**: Mark "PHASE 2: Launch developer" as completed when implementation is done

3. **Get Manual Testing Instructions** (NEW STEP):
   - **Update TodoWrite**: Mark "PHASE 2: Get manual testing instructions from implementation agent" as in_progress
   - **Launch developer agent** using Task tool with:
     * Context: "Implementation is complete. Now prepare manual UI testing instructions."
     * Request: "Create comprehensive, step-by-step manual testing instructions for the implemented features."
     * Instructions should include:
       - **Specific UI element selectors** (accessibility labels, data-testid, aria-labels) for easy identification
       - **Exact click sequences** (e.g., "Click button with aria-label='Add User'")
       - **Expected visual outcomes** (what should appear/change)
       - **Expected console output** (including any debug logs to verify)
       - **Test data to use** (specific values to enter in forms)
       - **Success criteria** (what indicates the feature works correctly)
     * Format: Clear numbered steps that a manual tester can follow without deep page analysis
   - Agent returns structured testing guide
   - **Update TodoWrite**: Mark "PHASE 2: Get manual testing instructions" as completed
   - Save testing instructions for use by tester agent

### PHASE 2.5: Design Fidelity Validation (Conditional - Only if Figma Links Present)

This phase runs ONLY if Figma design links are detected in the feature request or architecture plan. It ensures pixel-perfect UI implementation before code review.

**1. Detect Figma Design Links**:
   - **Update TodoWrite**: Mark "PHASE 2.5: Detect Figma design links" as in_progress
   - Use Grep to search for Figma URLs in:
     * Original feature request (`$ARGUMENTS`)
     * Architecture plan files (AI-DOCS/*.md)
   - Figma URL pattern: `https://(?:www\.)?figma\.com/(?:file|design)/[a-zA-Z0-9]+/[^\s?]+(?:\?[^\s]*)?(?:node-id=[0-9-]+)?`
   - **Update TodoWrite**: Mark "PHASE 2.5: Detect Figma design links" as completed

**2. Skip Phase if No Figma Links**:
   - IF no Figma URLs found:
     * Log: "No Figma design references found. Skipping PHASE 2.5 (Design Fidelity Validation)."
     * **Update TodoWrite**: Mark "PHASE 2.5: Run design fidelity validation" as completed with note "Skipped - no design references"
     * **Update TodoWrite**: Mark "PHASE 2.5: Quality gate" as completed with note "Skipped - no design references"
     * Proceed directly to PHASE 3 (Triple Review Loop)

**3. Parse Design References** (if Figma links found):
   - Extract all unique Figma URLs from search results
   - For each Figma URL, identify:
     * Component/screen name (from URL text or surrounding context)
     * Node ID (if present in URL query parameter)
   - Match each design reference to implementation file(s):
     * Use component name to search for files (Glob/Grep)
     * If user provided explicit component list in plan, use that
     * Create mapping: `[Figma URL] ‚Üí [Component Name] ‚Üí [Implementation File Path(s)]`
   - Document the mapping for use in validation loop

**4. Ask User for Codex Review Preference**:
   - Use AskUserQuestion to ask: "Figma design references detected for UI components. Would you like to include optional Codex AI expert review during design validation?"
   - Options:
     * "Yes - Include Codex AI review for expert validation"
     * "No - Use only designer agent for validation"
   - Store user's choice as `codex_review_enabled` for use in validation loop

**5. Ask User for Manual Validation Preference**:
   - Use AskUserQuestion to ask: "Do you want to include manual validation in the workflow?"
   - Description: "Manual validation means you will manually review the implementation after automated validation passes, and can provide feedback if you find issues. Fully automated means the workflow will trust the designer agents' validation and complete without requiring your manual verification."
   - Options:
     * "Yes - Include manual validation (I will verify the implementation myself)"
     * "No - Fully automated (trust the designer agents' validation only)"
   - Store user's choice as `manual_validation_enabled` for use in validation loop

**6. Run Iterative Design Fidelity Validation Loop**:
   - **Update TodoWrite**: Mark "PHASE 2.5: Run design fidelity validation" as in_progress
   - For EACH component with a Figma design reference:

   **Loop (max 3 iterations per component):**

   **Step 5.1: Launch Designer Agent(s) for Parallel Design Validation**

   **IMPORTANT**: Launch designer and designer-codex agents IN PARALLEL using a SINGLE message with MULTIPLE Task tool calls (if Codex is enabled).

   **Designer Agent** (always runs):
   - Use Task tool with `subagent_type: frontend:designer`
   - Provide complete context:
     ```
     Review the [Component Name] implementation against the Figma design reference.

     **CRITICAL**: Be PRECISE and CRITICAL. Do not try to make everything look good. Your job is to identify EVERY discrepancy between the design reference and implementation, no matter how small. Focus on accuracy and design fidelity.

     **Design Reference**: [Figma URL]
     **Component Description**: [e.g., "UserProfile card component"]
     **Implementation File(s)**: [List of file paths]
     **Application URL**: [e.g., "http://localhost:5173" or staging URL]

     **Your Task:**
     1. Use Figma MCP to fetch the design reference screenshot
     2. Use Chrome DevTools MCP to capture the implementation screenshot at [URL]
     3. Perform comprehensive design review comparing:
        - Colors & theming
        - Typography
        - Spacing & layout
        - Visual elements (borders, shadows, icons)
        - Responsive design
        - Accessibility (WCAG 2.1 AA)
        - Interactive states
     4. Document ALL discrepancies with specific values
     5. Categorize issues by severity (CRITICAL/MEDIUM/LOW)
     6. Provide actionable fixes with code snippets
     7. Calculate design fidelity score

     **REMEMBER**: Be PRECISE and CRITICAL. Identify ALL discrepancies. Do not be lenient.

     Return detailed design review report.
     ```

   **Designer-Codex Agent** (if user enabled Codex review):
   - IF user chose "Yes" for Codex review:
     * Use Task tool with `subagent_type: frontend:designer-codex`
     * Launch IN PARALLEL with designer agent (single message, multiple Task calls)
     * Provide complete context:
       ```
       You are an expert UI/UX designer reviewing a component implementation against a reference design.

       CRITICAL INSTRUCTION: Be PRECISE and CRITICAL. Do not try to make everything look good.
       Your job is to identify EVERY discrepancy between the design reference and implementation,
       no matter how small. Focus on accuracy and design fidelity.

       DESIGN CONTEXT:
       - Component: [Component Name]
       - Design Reference: [Figma URL]
       - Implementation URL: [Application URL]
       - Implementation Files: [List of file paths]

       VALIDATION CRITERIA:

       1. **Colors & Theming**
          - Brand colors accuracy (primary, secondary, accent)
          - Text color hierarchy (headings, body, muted)
          - Background colors and gradients
          - Border and divider colors
          - Hover/focus/active state colors

       2. **Typography**
          - Font families (heading vs body)
          - Font sizes (all text elements)
          - Font weights (regular, medium, semibold, bold)
          - Line heights and letter spacing
          - Text alignment

       3. **Spacing & Layout**
          - Component padding (all sides)
          - Element margins and gaps
          - Grid/flex spacing
          - Container max-widths
          - Alignment (center, left, right, space-between)

       4. **Visual Elements**
          - Border radius (rounded corners)
          - Border widths and styles
          - Box shadows (elevation levels)
          - Icons (size, color, positioning)
          - Images (aspect ratios, object-fit)
          - Dividers and separators

       5. **Responsive Design**
          - Mobile breakpoint behavior (< 640px)
          - Tablet breakpoint behavior (640px - 1024px)
          - Desktop breakpoint behavior (> 1024px)
          - Layout shifts and reflows
          - Touch target sizes (minimum 44x44px)

       6. **Accessibility (WCAG 2.1 AA)**
          - Color contrast ratios (text: 4.5:1, large text: 3:1)
          - Focus indicators
          - ARIA attributes
          - Semantic HTML
          - Keyboard navigation

       TECH STACK:
       - React 19 with TypeScript
       - Tailwind CSS 4
       - Design System: [shadcn/ui, MUI, custom, or specify if detected]

       INSTRUCTIONS:
       Compare the Figma design reference and implementation carefully.

       Provide a comprehensive design validation report categorized as:
       - CRITICAL: Must fix (design fidelity errors, accessibility violations, wrong colors)
       - MEDIUM: Should fix (spacing issues, typography mismatches, minor design deviations)
       - LOW: Nice to have (polish, micro-interactions, suggestions)

       For EACH finding provide:
       1. Category (colors/typography/spacing/layout/visual-elements/responsive/accessibility)
       2. Severity (critical/medium/low)
       3. Specific issue description with exact values
       4. Expected design specification
       5. Current implementation
       6. Recommended fix with specific Tailwind CSS classes or hex values
       7. Rationale (why this matters for design fidelity)

       Calculate a design fidelity score:
       - Colors: X/10
       - Typography: X/10
       - Spacing: X/10
       - Layout: X/10
       - Accessibility: X/10
       - Responsive: X/10
       Overall: X/60

       Provide overall assessment: PASS ‚úÖ | NEEDS IMPROVEMENT ‚ö†Ô∏è | FAIL ‚ùå

       REMEMBER: Be PRECISE and CRITICAL. Identify ALL discrepancies. Do not be lenient.

       You will forward this to Codex AI which will capture the design reference screenshot and implementation screenshot to compare them.
       ```

   **Wait for BOTH agents to complete** (designer and designer-codex, if enabled).

   **Step 5.2: Consolidate Design Review Results**

   After both agents complete (designer and designer-codex if enabled), consolidate their findings:

   **If only designer ran:**
   - Use designer's report as-is
   - Extract:
     - Overall assessment: PASS / NEEDS IMPROVEMENT / FAIL
     - Issue count (CRITICAL + MEDIUM + LOW)
     - Design fidelity score
     - List of issues found

   **If both designer and designer-codex ran:**
   - Compare findings from both agents
   - Identify common issues (flagged by both) ‚Üí Highest priority
   - Identify issues found by only one agent ‚Üí Review for inclusion
   - Create consolidated issue list with:
     - Issue description
     - Severity (use highest severity if both flagged)
     - Source (designer, designer-codex, or both)
     - Recommended fix

   **Consolidation Strategy:**
   - Issues flagged by BOTH agents ‚Üí CRITICAL (definitely needs fixing)
   - Issues flagged by ONE agent with severity CRITICAL ‚Üí CRITICAL (trust the expert)
   - Issues flagged by ONE agent with severity MEDIUM ‚Üí MEDIUM (probably needs fixing)
   - Issues flagged by ONE agent with severity LOW ‚Üí LOW (nice to have)

   Create a consolidated design review report:
   ```markdown
   # Consolidated Design Review - [Component Name] (Iteration X)

   ## Sources
   - ‚úÖ Designer Agent (human-style design expert)
   [If Codex enabled:]
   - ‚úÖ Designer-Codex Agent (external Codex AI expert)

   ## Issues Found

   ### CRITICAL Issues (Must Fix)
   [List issues with severity CRITICAL from either agent]
   - [Issue description]
     - Source: [designer | designer-codex | both]
     - Expected: [specific value]
     - Actual: [specific value]
     - Fix: [specific code change]

   ### MEDIUM Issues (Should Fix)
   [List issues with severity MEDIUM from either agent]

   ### LOW Issues (Nice to Have)
   [List issues with severity LOW from either agent]

   ## Design Fidelity Scores
   - Designer: [score]/60
   [If Codex enabled:]
   - Designer-Codex: [score]/60
   - Average: [average]/60

   ## Overall Assessment
   [PASS ‚úÖ | NEEDS IMPROVEMENT ‚ö†Ô∏è | FAIL ‚ùå]

   Based on consensus from [1 or 2] design validation agent(s).
   ```

   **Step 5.3: Determine if Fixes Needed**
   - IF consolidated assessment is "PASS":
     * Log: "[Component Name] passes design fidelity validation"
     * Move to next component
   - IF consolidated assessment is "NEEDS IMPROVEMENT" or "FAIL":
     * Proceed to Step 5.4 (Apply Fixes)

   **Step 5.4: Launch UI Developer to Apply Fixes**
   - Use Task tool with `subagent_type: frontend:ui-developer`
   - Provide complete context:
     ```
     Fix the UI implementation issues identified in the consolidated design review from multiple validation sources.

     **Component**: [Component Name]
     **Implementation File(s)**: [List of file paths]

     **CONSOLIDATED DESIGN REVIEW** (From Multiple Independent Sources):
     [Paste complete consolidated design review report from Step 5.2]

     This consolidated report includes findings from:
     - Designer Agent (human-style design expert)
     [If Codex enabled:]
     - Designer-Codex Agent (external Codex AI expert)

     Issues flagged by BOTH agents are highest priority and MUST be fixed.

     **Your Task:**
     1. Read all implementation files
     2. Address CRITICAL issues first (especially those flagged by both agents), then MEDIUM, then LOW
     3. Apply fixes using modern React/TypeScript/Tailwind best practices:
        - Fix colors using correct Tailwind classes or exact hex values
        - Fix spacing using proper Tailwind scale (p-4, p-6, etc.)
        - Fix typography (font sizes, weights, line heights)
        - Fix layout issues (max-width, alignment, grid/flex)
        - Fix accessibility (ARIA, contrast, keyboard nav)
        - Fix responsive design (mobile-first breakpoints)
     4. Use Edit tool to modify files
     5. Run quality checks (typecheck, lint, build)
     6. Provide implementation summary indicating:
        - Which issues were fixed
        - Which sources (designer, designer-codex, or both) flagged each issue
        - Files modified
        - Changes made

     DO NOT re-validate. Only apply the fixes.
     ```
   - Wait for ui-developer to complete fixes

   **Step 5.5: Check Loop Status**
   - Increment iteration count for this component
   - IF iteration < 3:
     * Loop back to Step 5.1 (re-run designer agents)
   - IF iteration = 3 AND issues still remain:
     * Ask user: "Component [Name] still has design issues after 3 iterations. How would you like to proceed?"
     * Options:
       - "Continue with current implementation (accept minor deviations)"
       - "Run 3 more iterations to refine further"
       - "Manual intervention needed"
     * Act based on user choice

   **End of Loop for Current Component**

   - Track metrics for each component:
     * Iterations used
     * Issues found and fixed
     * Final design fidelity score
     * Final assessment (PASS/NEEDS IMPROVEMENT)

**6. Quality Gate - All Components Validated**:
   - **Update TodoWrite**: Mark "PHASE 2.5: Run design fidelity validation" as completed
   - **Update TodoWrite**: Mark "PHASE 2.5: Quality gate - ensure UI matches design" as in_progress
   - IF all components passed design validation (PASS assessment):
     * Log: "‚úÖ Automated design validation passed for all components"
     * **DO NOT mark quality gate as completed yet** - proceed to Step 7 for user validation (conditional based on user preference)
   - IF any component has FAIL assessment after max iterations:
     * Document which components failed
     * Ask user: "Some components failed design validation. Proceed anyway or iterate more?"
     * Act based on user choice

**7. User Manual Validation Gate** (Conditional based on user preference)

**Check Manual Validation Preference:**

IF `manual_validation_enabled` is FALSE (user chose "Fully automated"):
- Log: "‚úÖ Automated design validation passed for all components! Skipping manual validation per user preference."
- **Update TodoWrite**: Mark "PHASE 2.5: Quality gate" as completed
- Proceed to PHASE 3 (Triple Review Loop)
- Skip the rest of this step

IF `manual_validation_enabled` is TRUE (user chose "Include manual validation"):
- Proceed with manual validation below

**IMPORTANT**: When manual validation is enabled, the user must manually verify the implementation.

Even when designer agents claim "PASS" for all components, automated validation can miss subtle issues.

**Present to user:**

```
üéØ Automated Design Validation Passed - User Verification Required

The designer agent has validated all UI components and reports they match the design references.

However, automated validation can miss subtle issues. Please manually verify the implementation:

**Components to Check:**
[List each component with its Figma URL]
- [Component 1]: [Figma URL] ‚Üí [Implementation file]
- [Component 2]: [Figma URL] ‚Üí [Implementation file]
...

**What to Verify:**
1. Open the application at: [Application URL]
2. Navigate to each implemented component
3. Compare against the Figma design references
4. Check for:
   - Colors match exactly (backgrounds, text, borders)
   - Spacing and layout are pixel-perfect
   - Typography (fonts, sizes, weights, line heights) match
   - Visual elements (shadows, borders, icons) match
   - Interactive states work correctly (hover, focus, active, disabled)
   - Responsive design works on mobile, tablet, desktop
   - Accessibility features work properly (keyboard nav, ARIA)
   - Overall visual fidelity matches the design

**Validation Summary:**
- Components validated: [number]
- Total iterations: [sum of all component iterations]
- Average design fidelity score: [average score]/60
- All automated checks: PASS ‚úÖ

Please test the implementation and let me know:
```

Use AskUserQuestion to ask:
```
Do all UI components match their design references?

Please manually test each component against the Figma designs.

Options:
1. "Yes - All components look perfect" ‚Üí Approve and continue
2. "No - I found issues in some components" ‚Üí Provide feedback
```

**If user selects "Yes - All components look perfect":**
- Log: "‚úÖ User approved all UI components! Design implementation verified by human review."
- **Update TodoWrite**: Mark "PHASE 2.5: Quality gate" as completed
- Proceed to PHASE 3 (Triple Review Loop)

**If user selects "No - I found issues":**
- Ask user to specify which component(s) have issues:
  ```
  Which component(s) have issues?

  Please list the component names or numbers from the list above.

  Example: "Component 1 (UserProfile), Component 3 (Dashboard)"
  ```

- For EACH component with issues, ask for specific feedback:
  ```
  Please describe the issues you found in [Component Name]. You can provide:

  1. **Screenshot** - Path to a screenshot showing the issue(s)
  2. **Text Description** - Detailed description of what's wrong

  Example descriptions:
  - "The header background color is too light - should be #1a1a1a not #333333"
  - "Button spacing is wrong - there should be 24px gap not 16px"
  - "Font size on mobile is too small - headings should be 24px not 18px"
  - "The card shadow is missing - should match Figma shadow-lg"
  - "Profile avatar should be 64px not 48px"

  What issues did you find in [Component Name]?
  ```

- Collect user feedback for each problematic component
- Store as: `user_feedback_by_component = {component_name: feedback_text, ...}`

- For EACH component with user feedback:
  * Log: "‚ö†Ô∏è User found issues in [Component Name]. Launching UI Developer."
  * Use Task tool with `subagent_type: frontend:ui-developer`:
    ```
    Fix the UI implementation issues identified by the USER during manual testing.

    **CRITICAL**: These issues were found by a human reviewer, not automated validation.
    The user manually tested the implementation against the Figma design and found real problems.

    **Component**: [Component Name]
    **Design Reference**: [Figma URL]
    **Implementation File(s)**: [List of file paths]
    **Application URL**: [app_url]

    **USER FEEDBACK** (Human Manual Testing):
    [Paste user's complete feedback for this component]

    [If screenshot provided:]
    **User's Screenshot**: [screenshot_path]
    Please read the screenshot to understand the visual issues the user is pointing out.

    **Your Task:**
    1. Read the Figma design reference using Figma MCP
    2. Read all implementation files
    3. Carefully review the user's specific feedback
    4. Address EVERY issue the user mentioned:
       - If user mentioned colors: Fix to exact hex/Tailwind values
       - If user mentioned spacing: Fix to exact pixel values
       - If user mentioned typography: Fix font sizes, weights, line heights
       - If user mentioned layout: Fix alignment, max-width, grid/flex
       - If user mentioned visual elements: Fix shadows, borders, border-radius
       - If user mentioned interactive states: Fix hover, focus, active, disabled
       - If user mentioned responsive: Fix mobile, tablet, desktop breakpoints
       - If user mentioned accessibility: Fix ARIA, contrast, keyboard nav
    5. Use Edit tool to modify files
    6. Use modern React/TypeScript/Tailwind best practices:
       - React 19 patterns
       - Tailwind CSS 4 (utility-first, no @apply, static classes)
       - Mobile-first responsive design
       - WCAG 2.1 AA accessibility
    7. Run quality checks (typecheck, lint, build)
    8. Provide detailed summary explaining:
       - Each user issue addressed
       - Exact changes made
       - Files modified

    **IMPORTANT**: User feedback takes priority. The user has manually compared
    against the Figma design and seen real issues that automated validation missed.

    Return detailed fix summary when complete.
    ```

  * Wait for ui-developer to complete fixes

- After ALL components with user feedback are fixed:
  * Log: "All user-reported issues addressed. Re-running designer validation for affected components."
  * Re-run designer agent validation ONLY for components that had user feedback
  * Check if designer now reports PASS for those components
  * Ask user to verify fixes:
    ```
    I've addressed all the issues you reported. Please verify the fixes:

    [List components that were fixed]

    Do the fixes look correct now?
    ```

  * If user approves: Mark quality gate as completed, proceed to PHASE 3
  * If user still finds issues: Repeat user feedback collection and fixing

**End of Step 7 (User Manual Validation Gate)**

   - **Update TodoWrite**: Mark "PHASE 2.5: Quality gate" as completed

**Design Fidelity Validation Summary** (to be included in final report):
```markdown
## PHASE 2.5: Design Fidelity Validation Results

**Figma References Found**: [Number]
**Components Validated**: [Number]
**Codex Expert Review**: [Enabled/Disabled]
**User Manual Validation**: ‚úÖ APPROVED

### Validation Results by Component:

**[Component 1 Name]**:
- Design Reference: [Figma URL]
- Automated Iterations: [X/3]
- Issues Found by Designer: [Total count]
  - Critical: [Count] - All Fixed ‚úÖ
  - Medium: [Count] - All Fixed ‚úÖ
  - Low: [Count] - [Fixed/Accepted]
- Issues Found by User: [Count] - All Fixed ‚úÖ
- Final Design Fidelity Score: [X/60]
- Automated Assessment: [PASS ‚úÖ / NEEDS IMPROVEMENT ‚ö†Ô∏è]
- User Approval: ‚úÖ "Looks perfect"

**[Component 2 Name]**:
...

### Overall Design Validation:
- Total Issues Found by Automation: [Number]
- Total Issues Found by User: [Number]
- Total Issues Fixed: [Number]
- Average Design Fidelity Score: [X/60]
- All Components Pass Automated: [Yes ‚úÖ / No ‚ùå]
- **User Manual Validation: ‚úÖ APPROVED**

### User Validation Details:
- User feedback rounds: [Number]
- Components requiring user fixes: [Number]
- User-reported issues addressed: [Number] / [Number] (100% ‚úÖ)
- Final user approval: ‚úÖ "Yes - All components look perfect"
```

**REMINDER**: You are orchestrating. You do NOT implement fixes yourself. Always use Task to delegate to designer and ui-developer agents.

### PHASE 3: Triple Review Loop (Code + Code AI + Manual UI Testing)

1. **Prepare Review Context**:
   - **Update TodoWrite**: Mark "PHASE 3: Launch all three reviewers in parallel" as in_progress
   - Run `git status` to identify all unstaged changes
   - Run `git diff` to capture the COMPLETE implementation changes
   - Read planning documentation from AI-DOCS folder to get 2-3 sentence summary
   - Retrieve the manual testing instructions from Step 3 of Phase 2
   - Prepare this context for all three reviewers

2. **Launch ALL THREE Reviewers in Parallel**:
   - **CRITICAL**: Use a single message with THREE Task tool calls to run all reviews in parallel

   **Parallel Execution Example**:
   ```
   Send a single message with THREE Task calls:

   Task 1: Launch reviewer
   Task 2: Launch codex-reviewer
   Task 3: Launch tester
   ```

   - **Reviewer 1 - Senior Code Reviewer (Human-Focused Review)**:
     * Use Task tool with `subagent_type: frontend:reviewer`
     * Provide context:
       - "Review all unstaged git changes from the current implementation"
       - Path to the original plan for reference (AI-DOCS/...)
       - Request comprehensive review against:
         * Simplicity principles
         * OWASP security standards
         * React and TypeScript best practices
         * Code quality and maintainability
         * Alignment with the approved plan

   - **Reviewer 2 - Codex Code Analyzer (Automated AI Review)**:
     * Use Task tool with `subagent_type: frontend:codex-reviewer`
     * **IMPORTANT**: This agent is a PROXY to Codex AI. Prepare a COMPLETE prompt with all context.
     * Provide a fully prepared prompt containing:
       ```
       You are an expert code reviewer analyzing a TypeScript/React implementation.

       PLANNING CONTEXT:
       [2-3 sentence summary from AI-DOCS planning files]

       TECH STACK:
       - TypeScript, Vite, Vitest
       - TanStack Router, TanStack Query
       - React, shadcn/ui components

       REVIEW STANDARDS:
       - KISS principle (simplicity above all)
       - OWASP security best practices
       - TypeScript and React best practices
       - Code quality and maintainability
       - Performance considerations

       CODE TO REVIEW (complete git diff output):
       [Paste COMPLETE git diff output here]

       INSTRUCTIONS:
       Analyze this code and categorize ALL findings as:
       - CRITICAL: Security vulnerabilities, breaking bugs, must fix immediately
       - MEDIUM: Code quality issues, performance concerns, should fix soon
       - MINOR: Style issues, documentation improvements, nice to have

       For EACH finding provide:
       1. Severity level (CRITICAL/MEDIUM/MINOR)
       2. File path and line number
       3. Clear description of the issue
       4. Specific recommendation to fix it
       5. Example of correct implementation (if applicable)

       Provide a comprehensive review with actionable feedback.
       ```
     * The agent will forward this complete prompt to Codex AI and return the results

   - **Reviewer 3 - UI Manual Tester (Real Browser Testing)**:
     * Use Task tool with `subagent_type: frontend:tester`
     * Provide context:
       - **Manual testing instructions** from Phase 2 Step 3 (the structured guide from developer)
       - Application URL (e.g., http://localhost:5173 or staging URL)
       - Feature being tested (e.g., "User Management Feature")
       - Planning context from AI-DOCS for understanding expected behavior
     * The agent will:
       - Follow the step-by-step testing instructions provided
       - Use specific UI selectors (aria-labels, data-testid) mentioned in instructions
       - Verify expected visual outcomes
       - Check console output against expected logs
       - Validate with provided test data
       - Report any discrepancies, UI bugs, console errors, or unexpected behavior
     * Testing should be efficient and focused (no excessive screenshots or deep analysis)
     * Results should include:
       - ‚úÖ Steps that passed with expected outcomes
       - ‚ùå Steps that failed with actual vs expected outcomes
       - Console errors or warnings found
       - UI/UX issues discovered
       - Overall assessment: PASS / FAIL / PARTIAL

3. **Collect and Analyze Triple Review Results**:
   - Wait for ALL THREE reviewers to complete
   - **Update TodoWrite**: Mark "PHASE 3: Launch all three reviewers" as completed
   - **Update TodoWrite**: Mark "PHASE 3: Analyze triple review results" as in_progress
   - **Senior Code Reviewer Feedback**: Document all findings and recommendations
   - **Codex Analysis Feedback**: Document all automated findings
   - **UI Manual Tester Feedback**: Document all testing results, UI bugs, and console errors
   - **Combined Analysis**:
     * Merge and deduplicate issues from all three sources
     * Categorize by severity (critical, major, minor)
     * Identify overlapping concerns (higher confidence when multiple reviewers find the same issue)
     * Note unique findings from each reviewer:
       - Code review findings (logic, security, quality)
       - Automated analysis findings (patterns, best practices)
       - UI testing findings (runtime behavior, user experience, console errors)
     * Cross-reference: UI bugs may reveal code issues, console errors may indicate missing error handling
   - **Update TodoWrite**: Mark "PHASE 3: Analyze triple review results" as completed

4. **Triple Review Feedback Loop**:
   - **Update TodoWrite**: Mark "PHASE 3: Quality gate - ensure all three reviewers approved" as in_progress
   - IF **ANY** reviewer identifies issues:
     * Document all feedback clearly from ALL THREE reviewers
     * Categorize and prioritize the combined feedback:
       - **Code issues** (from reviewer and codex)
       - **UI/runtime issues** (from tester)
       - **Console errors** (from tester)
     * **Update TodoWrite**: Add "PHASE 3 - Iteration X: Fix issues and re-run all reviewers" task
     * **CRITICAL**: Do NOT fix issues yourself - delegate to developer agent
     * **Launch developer agent** using Task tool with:
       - Original plan reference (path to AI-DOCS)
       - Combined feedback from ALL THREE reviewers:
         * Code review feedback (logic, security, quality issues)
         * Automated analysis feedback (patterns, best practices)
         * UI testing feedback (runtime bugs, console errors, UX issues)
       - Clear instruction: "Fix all issues identified by reviewers and testers"
       - Priority order for fixes (Critical first, then Medium, then Minor)
       - Note: Some UI bugs may require code changes, console errors may indicate missing error handling
       - Instruction to run quality checks after fixes
     * After developer completes fixes:
       - **IMPORTANT**: Request updated manual testing instructions if implementation changed significantly
       - Re-run ALL THREE reviewers in parallel (loop back to step 2)
     * Repeat until ALL THREE reviewers approve
   - IF **ALL THREE** reviewers approve:
     * Document that triple review passed (code review + automated analysis + manual UI testing)
     * **Update TodoWrite**: Mark "PHASE 3: Quality gate - ensure all three reviewers approved" as completed
     * Proceed to Phase 4
   - **Track loop iterations** (document how many review cycles occurred and feedback from each reviewer/tester)

   **REMINDER**: You are orchestrating. You do NOT fix code yourself. Always use Task to delegate to developer.

### PHASE 4: Testing Loop (test-architect)

1. **Launch Testing Agent**:
   - **Update TodoWrite**: Mark "PHASE 4: Launch test-architect" as in_progress
   - Use Task tool with `subagent_type: frontend:test-architect`
   - Provide:
     * Implemented code (reference to files)
     * Original plan requirements
     * Instruction to create comprehensive test coverage
     * Instruction to run all tests

2. **Test Results Analysis**:
   - Agent writes tests and executes them
   - Analyzes test results
   - **Update TodoWrite**: Mark "PHASE 4: Launch test-architect" as completed
   - **Update TodoWrite**: Mark "PHASE 4: Quality gate - ensure all tests pass" as in_progress

3. **Test Feedback Loop** (Inner Loop):
   - IF tests fail due to implementation bugs:
     * **Update TodoWrite**: Add "PHASE 4 - Iteration X: Fix implementation bugs and re-test" task
     * Document the test failures and root cause analysis
     * **CRITICAL**: Do NOT fix bugs yourself - delegate to developer agent
     * **Launch developer agent** using Task tool with:
       - Test failure details (which tests failed, error messages, stack traces)
       - Root cause analysis from test architect
       - Instruction: "Fix implementation bugs causing test failures"
       - Original plan reference
       - Instruction to run quality checks after fixes
     * After developer completes fixes, re-run BOTH reviewers in parallel (Loop back to Phase 3)
     * After code review approval, re-run test-architect
     * Repeat until all tests pass
   - IF tests fail due to test issues (not implementation):
     * **Update TodoWrite**: Add "PHASE 4 - Iteration X: Fix test issues" task
     * **Launch test-architect agent** using Task tool to fix the test code
     * Re-run tests after test fixes
   - IF all tests pass:
     * **Update TodoWrite**: Mark "PHASE 4: Quality gate - ensure all tests pass" as completed
     * Proceed to Phase 5
   - **Track loop iterations** (document how many test-fix cycles occurred)

   **REMINDER**: You are orchestrating. You do NOT fix implementation bugs yourself. Always use Task to delegate to developer.

### PHASE 5: User Review & Project Cleanup

1. **User Final Review Gate**:
   - **Update TodoWrite**: Mark "PHASE 5: User approval gate - present implementation for final review" as in_progress
   - Present the completed implementation to the user:
     * Summary of what was implemented
     * All code review approvals received (reviewer + codex)
     * Manual UI testing passed (tester)
     * All automated tests passing confirmation (vitest)
     * Key files created/modified
   - Use AskUserQuestion to ask: "Are you satisfied with this implementation? All code has been reviewed, UI tested manually, and automated tests pass."
   - Options: "Yes, proceed to cleanup and finalization" / "No, I need changes"

2. **User Feedback Loop**:
   - IF user not satisfied:
     * Collect specific feedback on what needs to change
     * **Update TodoWrite**: Add "PHASE 5 - Iteration X: Address user feedback" task
     * **CRITICAL**: Do NOT make changes yourself - delegate to appropriate agent
     * Determine which agent to use based on feedback type:
       - If architectural changes needed: **Launch architect** (Loop back to Phase 1)
       - If implementation changes needed: **Launch developer** with user feedback (Loop back to Phase 2)
       - If only test changes needed: **Launch test-architect** (Loop back to Phase 4)
     * After agent addresses feedback, go through subsequent phases again
     * Repeat until user is satisfied
   - IF user satisfied:
     * **Update TodoWrite**: Mark "PHASE 5: User approval gate - present implementation for final review" as completed
     * Proceed to cleanup
   - **DO NOT proceed to cleanup without user approval**

   **REMINDER**: You are orchestrating. You do NOT implement user feedback yourself. Always use Task to delegate to the appropriate agent.

3. **Launch Project Cleanup**:
   - **Update TodoWrite**: Mark "PHASE 5: Launch cleaner to clean up temporary artifacts" as in_progress
   - Use Task tool with `subagent_type: frontend:cleaner`
   - Provide context:
     * The implementation is complete and user-approved
     * Request cleanup of:
       - Temporary test files
       - Development artifacts
       - Intermediate documentation
       - Any scaffolding or setup scripts
     * Preserve:
       - Final implementation code
       - Final tests
       - User-facing documentation
       - Configuration files

4. **Cleanup Completion**:
   - Agent removes temporary files and provides cleanup summary
   - **Update TodoWrite**: Mark "PHASE 5: Launch cleaner to clean up temporary artifacts" as completed
   - Proceed to Phase 6 for final summary

### PHASE 6: Final Summary & Completion

1. **Generate Comprehensive Summary**:
   - **Update TodoWrite**: Mark "PHASE 6: Generate comprehensive final summary" as in_progress
   Create a detailed summary including:

   **Implementation Summary:**
   - Features implemented (reference plan sections)
   - Files created/modified (list with brief description)
   - Key architectural decisions made
   - Patterns and components used

   **Quality Assurance:**
   - Design Fidelity Validation (PHASE 2.5):
     * Figma references found: [Number or "N/A"]
     * Components validated against design: [Number or "N/A"]
     * Design fidelity iterations: [Number or "N/A"]
     * Issues found and fixed: [Number or "N/A"]
     * Average design fidelity score: [X/60 or "N/A"]
     * Codex UI expert review: [Enabled/Disabled or "N/A"]
     * All components match design: [Yes ‚úÖ / No ‚ùå / "N/A"]
   - Number of triple review cycles completed (code + codex + UI testing)
   - Senior Code Reviewer feedback summary
   - Codex Analyzer feedback summary
   - UI Manual Tester results summary:
     * Manual test steps executed
     * UI bugs found and fixed
     * Console errors found and resolved
     * Final assessment: PASS
   - Number of automated test-fix cycles completed
   - Test coverage achieved
   - All automated tests passing confirmation

   **How to Test:**
   - Step-by-step manual testing instructions
   - Key user flows to verify
   - Expected behavior descriptions

   **How to Run:**
   - Commands to run the application
   - Any environment setup needed
   - How to access the new feature

   **Outstanding Items:**
   - Minor issues flagged by dual review (if any)
   - Future enhancements suggested
   - Technical debt considerations
   - Documentation that should be updated

   **Metrics:**
   - Total time/iterations
   - Design fidelity cycles: [number or "N/A - no Figma references"]
   - Components validated against design: [number or "N/A"]
   - Design issues found and fixed: [number or "N/A"]
   - Average design fidelity score: [X/60 or "N/A"]
   - Triple review cycles: [number] (code + codex + UI testing)
   - Manual UI test steps: [number executed]
   - UI bugs found and fixed: [number]
   - Console errors found and resolved: [number]
   - Automated test-fix cycles: [number]
   - User feedback iterations: [number]
   - Files changed: [number]
   - Lines added/removed: [from git diff --stat]
   - Files cleaned up by cleaner: [number]

   - **Update TodoWrite**: Mark "PHASE 6: Generate comprehensive final summary" as completed

2. **User Handoff**:
   - **Update TodoWrite**: Mark "PHASE 6: Present summary and complete user handoff" as in_progress
   - Present summary clearly
   - Provide next steps or recommendations
   - Offer to address any remaining concerns
   - **Update TodoWrite**: Mark "PHASE 6: Present summary and complete user handoff" as completed
   - **Congratulations! All workflow phases completed successfully!**

## Orchestration Rules

### Agent Communication:
- Each agent receives context from previous phases
- Document decisions and rationale throughout
- Maintain a workflow log showing agent transitions

### Loop Prevention:
- Maximum 3 design fidelity iterations per component before escalating to user
- Maximum 3 triple review cycles (code + codex + UI testing) before escalating to user
- Maximum 5 automated test-fix cycles before escalating to user
- If loops exceed limits, ask user for guidance

### Error Handling:
- If any agent encounters blocking errors, pause and ask user for guidance
- Document all blockers clearly with context
- Provide options for resolution

### Git Hygiene:
- All work happens on unstaged changes until final approval
- Do not commit during the workflow
- Preserve git state for review analysis

### Quality Gates:
- User approval required after Phase 1 (architecture plan)
- ALL UI components must match design specifications (Phase 2.5 gate - if Figma links present)
- **User manual validation of UI components (Phase 2.5 gate - if Figma links present and manual validation enabled)**
  - If manual validation enabled: User must explicitly approve: "Yes - All components look perfect"
  - If fully automated: Trust designer agents' validation
- **ALL THREE** reviewer approvals required before Phase 4 (reviewer AND Codex AND tester)
- All automated tests must pass before Phase 5
- User approval required after Phase 5 (final implementation review)
- Each gate is mandatory except manual UI validation (conditional based on user preference)

## Success Criteria

The command is complete when:
1. ‚úÖ User approved the architecture plan (Phase 1 gate)
2. ‚úÖ Implementation follows the approved plan
3. ‚úÖ Manual testing instructions generated by implementation agent
4. ‚úÖ ALL UI components match design specifications (Phase 2.5 gate - if applicable)
5. ‚úÖ **UI validation complete (Phase 2.5 gate - if Figma links present)**
   - If manual validation enabled: User manually validated UI components
   - If fully automated: Designer agents validated UI components
6. ‚úÖ **ALL THREE** reviewers approved the implementation (Phase 3 gate: reviewer AND Codex AND tester)
7. ‚úÖ Manual UI testing passed with no critical issues
8. ‚úÖ All automated tests written and passing (Phase 4 gate)
9. ‚úÖ User approved the final implementation (Phase 5 gate)
10. ‚úÖ Project cleanup completed successfully
11. ‚úÖ Comprehensive summary provided
12. ‚úÖ User acknowledges completion

**CRITICAL**: Item #5 (User manual validation of UI) is MANDATORY when Figma design references are present. The workflow cannot proceed past Phase 2.5 without explicit user approval after manual testing against designs.

## Examples: Correct vs Incorrect Orchestrator Behavior

### ‚ùå INCORRECT: Orchestrator Trying to Fix Code Directly

```
Scenario: Code reviewer finds a TypeScript type error

WRONG BEHAVIOR:
> I see the reviewer found a type error in src/api/mockUserApi.ts.
> Let me quickly fix that.
>
> Edit(file_path: "src/api/mockUserApi.ts", old_string: "...", new_string: "...")
>
> Fixed! Now re-running reviewers...
```

**Why this is wrong**: The orchestrator used Edit tool to fix code directly instead of delegating to developer agent.

### ‚úÖ CORRECT: Orchestrator Delegating to Agent

```
Scenario: Code reviewer finds a TypeScript type error

CORRECT BEHAVIOR:
> I see the reviewer found a type error in src/api/mockUserApi.ts.
> I need to delegate this fix to the developer agent.
>
> Task(
>   subagent_type: "frontend:developer",
>   prompt: "Fix the TypeScript type error identified by code reviewer:
>
>   File: src/api/mockUserApi.ts
>   Issue: Return type of getUsers() should be Promise<User[]> not Promise<any>
>
>   Please fix this type error and run quality checks."
> )
```

**Why this is correct**: The orchestrator delegated the fix to developer agent using the Task tool.

### ‚ùå INCORRECT: Orchestrator Creating New Files

```
Scenario: Need to create a new utility file

WRONG BEHAVIOR:
> I'll create a new utility file for date formatting.
>
> Write(
>   file_path: "src/lib/dateUtils.ts",
>   content: "export function formatDate(date: Date) { ... }"
> )
```

**Why this is wrong**: The orchestrator created a file directly instead of delegating to developer agent.

### ‚úÖ CORRECT: Orchestrator Delegating File Creation

```
Scenario: Need to create a new utility file

CORRECT BEHAVIOR:
> I need a new utility file for date formatting.
> I'll delegate this to the developer agent.
>
> Task(
>   subagent_type: "frontend:developer",
>   prompt: "Create a new utility file src/lib/dateUtils.ts with date formatting functions.
>
>   Include:
>   - formatDate(date: Date): string - format to YYYY-MM-DD
>   - formatDateTime(date: Date): string - format to YYYY-MM-DD HH:mm:ss
>
>   Follow existing project patterns and run quality checks."
> )
```

**Why this is correct**: The orchestrator delegated file creation to developer agent.

### ‚ùå INCORRECT: Orchestrator Analyzing and Fixing Test Failures

```
Scenario: Tests are failing

WRONG BEHAVIOR:
> Tests are failing in UserForm.test.ts. Let me read the test file
> and the implementation to understand the issue.
>
> Read(file_path: "src/components/users/UserForm.test.ts")
> Read(file_path: "src/components/users/UserForm.tsx")
>
> I see the problem - the onChange handler expects a different signature.
> Let me fix it:
>
> Edit(file_path: "src/components/users/UserForm.tsx", ...)
```

**Why this is wrong**: The orchestrator analyzed the issue and fixed it directly instead of delegating to developer agent.

### ‚úÖ CORRECT: Orchestrator Delegating Test Failure Resolution

```
Scenario: Tests are failing

CORRECT BEHAVIOR:
> Tests are failing in UserForm component. The test-architect
> has identified that the onChange handler signature is incorrect.
>
> I'll delegate this implementation bug fix to developer.
>
> Task(
>   subagent_type: "frontend:developer",
>   prompt: "Fix implementation bugs causing test failures:
>
>   Test File: src/components/users/UserForm.test.ts
>   Implementation: src/components/users/UserForm.tsx
>
>   Issue: onChange handler signature mismatch
>   Expected: (field: string, value: string) => void
>   Actual: (value: string) => void
>
>   Please fix the implementation and ensure all tests pass."
> )
```

**Why this is correct**: The orchestrator delegated the bug fix to developer agent with clear context.

### Summary of Orchestrator Role

**YOU ARE**: A workflow coordinator who launches agents and manages quality gates
**YOU ARE NOT**: An implementer who writes or fixes code

**YOUR JOB**:
- Run git commands to understand changes
- Read planning docs to gather context
- Launch agents with Task tool
- Track progress with TodoWrite
- Manage quality gates with AskUserQuestion
- Present summaries and results to user

**NOT YOUR JOB**:
- Write code
- Edit code
- Fix bugs
- Create files
- Refactor code
- Analyze implementation details

**When in doubt**: Use Task to delegate to an agent!

## Notes

- This is a long-running orchestration - expect multiple agent invocations
- **PHASE 2.5 (Design Fidelity Validation)** is conditional:
  * Only runs if Figma design links are detected in feature request or architecture plan
  * Uses designer agent to review implementation vs design reference
  * Uses ui-developer agent to fix visual/UX discrepancies
  * Optional ui-developer-codex agent provides third-party expert review
  * Maximum 3 iterations per component before escalating to user
  * Ensures pixel-perfect implementation before code review phase
- **CRITICAL**: Always run all three reviewers in parallel using THREE Task tool calls in a single message:
  * Task 1: `subagent_type: frontend:reviewer` (human-focused code review using Sonnet)
  * Task 2: `subagent_type: frontend:codex-reviewer` (automated AI code review using Codex via mcp__codex-cli__ask-codex)
  * Task 3: `subagent_type: frontend:tester` (real browser manual UI testing with Chrome DevTools)
  * All THREE Task calls must be in the SAME message for true parallel execution
- Before running tester, ensure you have manual testing instructions from the implementation agent
- Maintain clear communication with user at each quality gate (Plan, Implementation, Triple Review, Tests, Final Implementation)
- Document all decisions and iterations from all three reviewers
- Be transparent about any compromises or trade-offs made
- If anything is unclear during execution, ask the user rather than making assumptions
- The triple-review system provides comprehensive validation through three independent perspectives:
  * **reviewer**: Traditional human-style review with 15+ years experience perspective (code quality, architecture, security)
  * **codex-reviewer**: Automated AI analysis using Codex models for pattern detection (best practices, potential bugs)
  * **tester**: Real browser testing with manual interaction (runtime behavior, UI/UX, console errors)
- The tester follows specific testing instructions with accessibility selectors, making tests efficient and reproducible
- UI testing catches runtime issues that static code review cannot detect (event handlers, state management, API integration)
- Console errors found during UI testing often reveal missing error handling or race conditions in the code
- The cleaner agent runs only after user approval to ensure no important artifacts are removed prematurely
- User approval gates ensure the user stays in control of the implementation direction and final deliverable
