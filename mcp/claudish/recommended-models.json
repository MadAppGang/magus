{
  "version": "1.1.5",
  "lastUpdated": "2025-11-19",
  "source": "https://openrouter.ai/api/v1/models",
  "models": [
    {
      "id": "x-ai/grok-4-fast",
      "name": "xAI: Grok 4 Fast",
      "description": "Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning. Read more about the model on xAI's [news post](http://x.ai/news/grok-4-fast). Reasoning can be enabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)",
      "provider": "X-ai",
      "category": "vision",
      "priority": 1,
      "pricing": {
        "input": "$0.20/1M",
        "output": "$0.50/1M",
        "average": "$0.35/1M"
      },
      "context": "2000K",
      "maxOutputTokens": 30000,
      "modality": "text+image->text",
      "supportsTools": true,
      "supportsReasoning": true,
      "supportsVision": true,
      "isModerated": false,
      "recommended": true
    },
    {
      "id": "openrouter/auto",
      "name": "Auto Router",
      "description": "Your prompt will be processed by a meta-model and routed to one of dozens of models (see below), optimizing for the best possible output.\n\nTo see which model was used, visit [Activity](/activity), or read the `model` attribute of the response. Your response will be priced at the same rate as the routed model.\n\nThe meta-model is powered by [Not Diamond](https://docs.notdiamond.ai/docs/how-not-diamond-works). Learn more in our [docs](/docs/model-routing).\n\nRequests will be routed to the following models:\n- [openai/gpt-5](/openai/gpt-5)\n- [openai/gpt-5-mini](/openai/gpt-5-mini)\n- [openai/gpt-5-nano](/openai/gpt-5-nano)\n- [openai/gpt-4.1-nano](/openai/gpt-4.1-nano)\n- [openai/gpt-4.1](/openai/gpt-4.1)\n- [openai/gpt-4.1-mini](/openai/gpt-4.1-mini)\n- [openai/gpt-4.1](/openai/gpt-4.1)\n- [openai/gpt-4o-mini](/openai/gpt-4o-mini)\n- [openai/chatgpt-4o-latest](/openai/chatgpt-4o-latest)\n- [anthropic/claude-3.5-haiku](/anthropic/claude-3.5-haiku)\n- [anthropic/claude-opus-4-1](/anthropic/claude-opus-4-1)\n- [anthropic/claude-sonnet-4-0](/anthropic/claude-sonnet-4-0)\n- [anthropic/claude-3-7-sonnet-latest](/anthropic/claude-3-7-sonnet-latest)\n- [google/gemini-2.5-pro](/google/gemini-2.5-pro)\n- [google/gemini-2.5-flash](/google/gemini-2.5-flash)\n- [mistral/mistral-large-latest](/mistral/mistral-large-latest)\n- [mistral/mistral-medium-latest](/mistral/mistral-medium-latest)\n- [mistral/mistral-small-latest](/mistral/mistral-small-latest)\n- [mistralai/mistral-nemo](/mistralai/mistral-nemo)\n- [x-ai/grok-3](/x-ai/grok-3)\n- [x-ai/grok-3-mini](/x-ai/grok-3-mini)\n- [x-ai/grok-4](/x-ai/grok-4)\n- [deepseek/deepseek-r1](/deepseek/deepseek-r1)\n- [meta-llama/llama-3.1-70b-instruct](/meta-llama/llama-3.1-70b-instruct)\n- [meta-llama/llama-3.1-405b-instruct](/meta-llama/llama-3.1-405b-instruct)\n- [mistralai/mixtral-8x22b-instruct](/mistralai/mixtral-8x22b-instruct)\n- [perplexity/sonar](/perplexity/sonar)\n- [cohere/command-r-plus](/cohere/command-r-plus)\n- [cohere/command-r](/cohere/command-r)",
      "provider": "Openrouter",
      "category": "reasoning",
      "priority": 2,
      "pricing": {
        "input": "$-1000000.00/1M",
        "output": "$-1000000.00/1M",
        "average": "$-1000000.00/1M"
      },
      "context": "2000K",
      "maxOutputTokens": null,
      "modality": "text->text",
      "supportsTools": false,
      "supportsReasoning": false,
      "supportsVision": false,
      "isModerated": false,
      "recommended": true
    },
    {
      "id": "google/gemini-3-pro-preview",
      "name": "Google: Gemini 3 Pro Preview",
      "description": "Gemini 3 Pro is Google’s flagship frontier model for high-precision multimodal reasoning, combining strong performance across text, image, video, audio, and code with a 1M-token context window. Reasoning Details must be preserved when using multi-turn tool calling, see our docs here: https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks. It delivers state-of-the-art benchmark results in general reasoning, STEM problem solving, factual QA, and multimodal understanding, including leading scores on LMArena, GPQA Diamond, MathArena Apex, MMMU-Pro, and Video-MMMU. Interactions emphasize depth and interpretability: the model is designed to infer intent with minimal prompting and produce direct, insight-focused responses.\n\nBuilt for advanced development and agentic workflows, Gemini 3 Pro provides robust tool-calling, long-horizon planning stability, and strong zero-shot generation for complex UI, visualization, and coding tasks. It excels at agentic coding (SWE-Bench Verified, Terminal-Bench 2.0), multimodal analysis, and structured long-form tasks such as research synthesis, planning, and interactive learning experiences. Suitable applications include autonomous agents, coding assistants, multimodal analytics, scientific reasoning, and high-context information processing.",
      "provider": "Google",
      "category": "coding",
      "priority": 3,
      "pricing": {
        "input": "$2.00/1M",
        "output": "$12.00/1M",
        "average": "$7.00/1M"
      },
      "context": "1048K",
      "maxOutputTokens": 65536,
      "modality": "text+image->text",
      "supportsTools": true,
      "supportsReasoning": true,
      "supportsVision": true,
      "isModerated": false,
      "recommended": true
    },
    {
      "id": "moonshotai/kimi-linear-48b-a3b-instruct",
      "name": "MoonshotAI: Kimi Linear 48B A3B Instruct",
      "description": "Kimi Linear is a hybrid linear attention architecture that outperforms traditional full attention methods across various contexts, including short, long, and reinforcement learning (RL) scaling regimes. At its core is Kimi Delta Attention (KDA)—a refined version of Gated DeltaNet that introduces a more efficient gating mechanism to optimize the use of finite-state RNN memory.\n\nKimi Linear achieves superior performance and hardware efficiency, especially for long-context tasks. It reduces the need for large KV caches by up to 75% and boosts decoding throughput by up to 6x for contexts as long as 1M tokens.",
      "provider": "Moonshotai",
      "category": "coding",
      "priority": 4,
      "pricing": {
        "input": "$0.50/1M",
        "output": "$0.60/1M",
        "average": "$0.55/1M"
      },
      "context": "1048K",
      "maxOutputTokens": 1048576,
      "modality": "text->text",
      "supportsTools": false,
      "supportsReasoning": false,
      "supportsVision": false,
      "isModerated": false,
      "recommended": true
    },
    {
      "id": "meta-llama/llama-4-maverick",
      "name": "Meta: Llama 4 Maverick",
      "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.\n\nMaverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.",
      "provider": "Meta-llama",
      "category": "coding",
      "priority": 5,
      "pricing": {
        "input": "$0.15/1M",
        "output": "$0.60/1M",
        "average": "$0.38/1M"
      },
      "context": "1048K",
      "maxOutputTokens": 16384,
      "modality": "text+image->text",
      "supportsTools": true,
      "supportsReasoning": false,
      "supportsVision": true,
      "isModerated": false,
      "recommended": true
    },
    {
      "id": "openai/gpt-4.1",
      "name": "OpenAI: GPT-4.1",
      "description": "GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.",
      "provider": "Openai",
      "category": "coding",
      "priority": 6,
      "pricing": {
        "input": "$2.00/1M",
        "output": "$8.00/1M",
        "average": "$5.00/1M"
      },
      "context": "1047K",
      "maxOutputTokens": 32768,
      "modality": "text+image->text",
      "supportsTools": true,
      "supportsReasoning": false,
      "supportsVision": true,
      "isModerated": true,
      "recommended": true
    },
    {
      "id": "minimax/minimax-01",
      "name": "MiniMax: MiniMax-01",
      "description": "MiniMax-01 is a combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding. It has 456 billion parameters, with 45.9 billion parameters activated per inference, and can handle a context of up to 4 million tokens.\n\nThe text model adopts a hybrid architecture that combines Lightning Attention, Softmax Attention, and Mixture-of-Experts (MoE). The image model adopts the “ViT-MLP-LLM” framework and is trained on top of the text model.\n\nTo read more about the release, see: https://www.minimaxi.com/en/news/minimax-01-series-2",
      "provider": "Minimax",
      "category": "vision",
      "priority": 7,
      "pricing": {
        "input": "$0.20/1M",
        "output": "$1.10/1M",
        "average": "$0.65/1M"
      },
      "context": "1000K",
      "maxOutputTokens": 1000192,
      "modality": "text+image->text",
      "supportsTools": false,
      "supportsReasoning": false,
      "supportsVision": true,
      "isModerated": false,
      "recommended": true
    },
    {
      "id": "amazon/nova-premier-v1",
      "name": "Amazon: Nova Premier 1.0",
      "description": "Amazon Nova Premier is the most capable of Amazon’s multimodal models for complex reasoning tasks and for use as the best teacher for distilling custom models.",
      "provider": "Amazon",
      "category": "vision",
      "priority": 8,
      "pricing": {
        "input": "$2.50/1M",
        "output": "$12.50/1M",
        "average": "$7.50/1M"
      },
      "context": "1000K",
      "maxOutputTokens": 32000,
      "modality": "text+image->text",
      "supportsTools": true,
      "supportsReasoning": false,
      "supportsVision": true,
      "isModerated": true,
      "recommended": true
    },
    {
      "id": "qwen/qwen-plus-2025-07-28",
      "name": "Qwen: Qwen Plus 0728",
      "description": "Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model with a balanced performance, speed, and cost combination.",
      "provider": "Qwen",
      "category": "reasoning",
      "priority": 9,
      "pricing": {
        "input": "$0.40/1M",
        "output": "$1.20/1M",
        "average": "$0.80/1M"
      },
      "context": "1000K",
      "maxOutputTokens": 32768,
      "modality": "text->text",
      "supportsTools": true,
      "supportsReasoning": false,
      "supportsVision": false,
      "isModerated": false,
      "recommended": true
    }
  ]
}